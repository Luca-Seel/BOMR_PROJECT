











!pip install tdmclient numpy scipy opencv-python tqdm



import numpy as np
from scipy.ndimage import distance_transform_edt
import cv2
import tqdm
from Computer_vision import cv as com
#import Filtering as flt 
from Global_Nav import global_nav as gb
import matplotlib.pyplot as plt

import math as m
import numpy as np
from collections import deque
from matplotlib.animation import FuncAnimation

from Filtering import Control_fromEKF as control
from Filtering import Filtering as filt




# Initiate the communication between the Thymio and the computer
from tdmclient import ClientAsync, aw
client = ClientAsync()
node = await client.wait_for_node()
await node.lock() 


def length_real_to_pixel(length_cm, env_size_cm, map_size):
    
    L, W = env_size_cm
    height, width = map_size
    
    # scale factors
    scale_x = width / L
    scale_y = height / W
    
    # average scale for uniform scaling
    scale = (scale_x + scale_y) / 2
    
    return length_cm * scale
    
#================HYPERPARAMETESRS======================

#SiZE OF THE ENVIRONEMENT (cm)
L = 113.5
W = 81.3
Ratio = L/W

#SiZE IN PIXEL OF THE CORRECTED MAP
Y_res = 480
SIZE = [Y_res, int(Y_res*Ratio)]

#SIZE OF THE TRIANGLE (cm)
L_T = 10
H_T = 3.5
L_T_p = length_real_to_pixel(L_T,[L,W], SIZE)
H_T_p = length_real_to_pixel(H_T,[L,W], SIZE)

#SIZE OF THE GOAL (cm)
R_G = 8.4/2

R_G_p = length_real_to_pixel(R_G,[L,W], SIZE)

# AREA RATIO
RATIO_T = (L_T*H_T/2) / (L*W) 

#FILTERING PARAMETERS FOR THE ORIGINAL IMAGE
B1_O = 10
B2_O = 80
B3_O = 80
C1_O = 0.5
C2_O = 1.5
M1_O = 8
CL1_O = 2.0
M_O = 5
D_O = 5

#FILTERING PARAMETERS FOR THE CORRECTED IMAGE
B1_C = 5
B2_C = 8
B3_C = 8
C1_C = 0.4
C2_C = 1.6
M1_C = 5
CL1_C = 4.0
M_C = 5
D_C = 5

#FILTERING PARAMETERS FOR ROBOT DETECTION
B1_T = 5
B2_T = 10
B3_T = 10
C1_T = 0.4
C2_T = 1.6
M1_T = 5
CL1_T = 2
M_T = 2
D_T = 2


#PARAMETERS TO DETECT THE ENV
Min_area_env = 1920*1080*0.4   #Min area in pixel of the env
Env_approx = 0.01

#PARAMETERS TO DETECT THE OBSTACLES
Min_area_obs = SIZE[0]*SIZE[1]*0.005   #Min area in pixel of the env
Max_area_obs = SIZE[0]*SIZE[1]*0.5   #Max area in pixel of the env
Obs_approx = 0.01

#PARAMETERS TO DETECT GOAL
Min_area_goal = np.pi * R_G_p * R_G_p *0.8
Max_area_goal = np.pi * R_G_p * R_G_p * 1.8

#PARAMETERS TO DETECT THE ROBOT
Min_area_rob = (L_T_p*H_T_p /2) * 0.8   #Min area in pixel of the env
Max_area_rob =  (L_T_p*H_T_p /2) * 1.2
Rob_approx = 0.08

#ROBOT CARACTERISTICS
R_ROBOT = 6
R_ROBOT_p = length_real_to_pixel(R_ROBOT,[L,W], SIZE)


#SYMBOLS FOR THE MAP
FREE_SPACE = 0
OCCUPIED_SPACE = -1
GOAL = -3

DEBUG = False
#========================================================


if DEBUG : 
    import matplotlib.pyplot as plt
    %matplotlib inline
#GLOBAL FRAMEWORK TO USE VISION
#EXECUTE CELL ABOVE WITH DEBUG = TRUE FOR TUNING UNTIL EVERYTHING IS CORRECTLY FILTERED AND FOUND
cap = cv2.VideoCapture(0)
    
if not cap.isOpened():
    print("Could not access webcam")







#INITIALIZATION
#GET A FRAME
img = com.get_picture_optimized(cap)
#img = cv2.imread("02.png")
#COMPUTE TRANSFORMATION MATRIX
matrix = com.matrix_perspective(img)
#CORRECT IMAGE
transformed_image = com.convert_perspective(img,matrix)

#PLOTTING
if DEBUG: 
    plt.figure()
    plt.imshow(cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))
    plt.title("Perspective Corrected Image")
    plt.axis('off')
    plt.show()


#GET GLOBAL MAP
global_map = com.get_map(transformed_image)

#GET ROBOT POSITION AND ORIENTATION
robot = com.get_robot(transformed_image)


length = 100
end_x = int(robot[0][0] + length * np.cos(robot[1]))
end_y = int(robot[0][1] + length * np.sin(robot[1]))
end_point = (end_x, end_y)
global_map4plot = global_map.copy()
cv2.arrowedLine(global_map4plot, [int(robot[0][0]), int(robot[0][1])], end_point, color=(2, 2, 2), thickness=10)
plt.imshow(global_map4plot)
plt.show()
if DEBUG : 
    global_map4plot = global_map.copy()
    cv2.arrowedLine(global_map4plot, robot[0], end_point, color=(2, 2, 2), thickness=10)
    plt.imshow(global_map4plot)
    plt.axis('off')
    plt.show()



#global_map[global_map == 1.] = -1.
global_map = global_map.astype(int)
from matplotlib import pyplot as plt
plt.imshow(global_map)
plt.axis('off')
plt.show()
print(global_map)
print(np.unique(global_map))


print(robot[0][1])


robot[0][0]


print(len(global_map[0]))


print(len(global_map))



int(round(robot[0][1]))


int(round(robot[0][0]))


solved = global_map[:]

path = path = gb.a_star(global_map, robot[1], 0.5, 
                 (int(round(robot[0][1])), int(round(robot[0][0]))))



## HERE : make infinite loop that launches Filtering, motion and local avoidance
#empty = np.zeros((SIZE[0], SIZE[1]))
for i in path : 
    global_map[i[0]][i[1]] = -4 # marker for path in my debug functions

gb.debug_maze_to_bitmap(global_map, 20, filename="Global_Nav/tests/maze.png")


# params ekf
Ts = 0.1  # time step in seconds
L = 95  # distance between wheels in mm
speed_to_mms = 0.3375  # conversion factor from thymio speed units to mm/s from solution ex.8 (in our measurement it was 0.43478260869565216)

# Process noise for EKF (tune) (from model-mismatch/random-walk/control execution)
q_proc = (
    1e-3, 1e-2, 1e-4,   # q_x, q_y, q_theta (model mismatch)
    75.72,  0.002692,         # q_v_ctrl, q_omega_ctrl (control execution noise)
    1e-2, 1e-5          # q_v_bias, q_omega_bias (random walk on v, omega)
)
# Camera measurement noise (tune)
r_cam = (1.435, 1.864, 0.001496)  # [mm^2, mm^2, rad^2]
r_mot = (75.72, 0.002692)    # motor noise on v, omega


# 1) buffers
traj = deque(maxlen=2000)   # (x,y)
# convert path!
waypoints = filt.remove_collinear(filt.grid_to_mm(path, cell_size_mm=5))
way_mm = filt.grid_to_mm(path, cell_size_mm=5)
# init EKF
ekf = filt.EKFState(x0=[0,0,0,0,0], P0=1000*np.eye(5))
#print(ekf)
ekf_traj = [] # for plot
# memory for plots
def ekf_get_state():
    s = ekf.get_state()  # (x,y,theta)
    ekf_traj.append((s[0], s[1], s[2]))  # log x,y each time it's called
    return s
image = []

# start connect to Thymio
#client = ClientAsync()
#node = aw(client.wait_for_node())
try:
    aw(node.lock()) # lock the node for R/W
except Exception:
    pass # ignore it it wasn't locked

aw(node.stop())
print("Connected:", node)
motors = aw(node.wait_for_variables({"motor.left.speed","motor.right.speed"}))

# helper functions:
def get_motor_meas(): 
    # raw speeds in Thymio units (instantaneous)
    vl = int(node.v.motor.left.speed)
    vr = int(node.v.motor.right.speed)
    print("get_motor_meas", vl, vr)
    # convert to v [mm/s], omega [rad/s] 
    v, w = control.motors_to_vw(vl, vr, speed_to_mms, L) 
    return np.array([v, w], dtype=float)

def get_cam_meas(image=None):
    # get position from camera
    if image is not None:
        pos, angle, __ = com.get_robot(image)
        return [pos, angle]
    return None


try:
    while waypoints:
        #GET image
        img = com.get_picture_optimized(cap)
        matrix = com.matrix_perspective(img)
        image = com.convert_perspective(img,matrix)
        # get motion params
        vl_cmd, vr_cmd = control.get_cmd()
        z_mot=get_motor_meas()
        z_cam = get_cam_meas(image)
        # calc ekf step
        ekf.step(vl_cmd, vr_cmd, z_cam, r_cam=r_cam, z_mot=z_mot, r_mot=r_mot, Ts=Ts, q_proc=q_proc)
        #print("ekf results:", ekf.x, ekf.P)
        state = ekf_get_state()
        # motion control
        print("waypoints:", waypoints)
        waypoints = aw(control.follow_path(node, state, waypoints, v_cmd=50, kp_heading=250.0,
                      pos_tol=12.0))
        
    
    x, y, th = ekf_get_state()[:3]
    print(f"Final pose (EKF): x={x:.1f} mm, y={y:.1f} mm, th={m.degrees(th):.1f} deg")
finally:
    aw(node.set_variables({"motor.left.target":[0], "motor.right.target":[0]}))
    aw(node.unlock())
    # plots
    # Build arrays
    if len(ekf_traj) > 1:
        xs, ys, th = zip(*ekf_traj)
    else:
        xs, ys = [], []

    dxs, dys = zip(*way_mm)

    plt.figure(figsize=(6,6))
    # desired path
    plt.plot(dxs, dys, 'k--', lw=2, label='Desired path (waypoints)')
    plt.scatter(dxs, dys, c='k', s=20)

    # actual path from EKF
    plt.plot(xs, ys, 'r-', lw=2, label='EKF trajectory')
    if xs and ys:
        plt.scatter([xs[0]],[ys[0]], c='g', s=40, label='Start (EKF)')
        plt.scatter([xs[-1]],[ys[-1]], c='r', s=40, label='End (EKF)')

    plt.axis('equal')
    plt.xlabel('x [mm]')
    plt.ylabel('y [mm]')
    plt.title('Desired vs EKF trajectory')
    plt.legend()
    plt.grid(True)
    plt.show()






